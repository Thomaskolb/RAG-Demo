{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/tkolb/data/envs/rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys,os,argparse\n",
    "sys.path.append('atlas/')\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.distributed as dist\n",
    "from src import dist_utils, slurm, util\n",
    "from src.index_io import load_or_initialize_index, save_embeddings_and_index\n",
    "from src.model_io import create_checkpoint_directories, load_or_initialize_atlas_model\n",
    "from src.options import get_options\n",
    "from src.tasks import get_task\n",
    "from evaluate import run_retrieval_only, _get_eval_data_iterator\n",
    "import types\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = './atlas_data'\n",
    "MODEL_PATH = '/home/tkolb/data/models/atlas/base'\n",
    "ATLAS_INDEX_PATH = '/home/tkolb/data/indices/atlas/wiki/base'\n",
    "EVAL_PATH = '/home/tkolb/data/datasets/queries/qs.jsonl'\n",
    "FAISS_INDEX_PATH = '/home/tkolb/data/faiss_index.index'\n",
    "PASSAGES_PATH = f'/home/tkolb/data/wiki_passages.pkl'\n",
    "DUCKDB_FAISS_EXT = 'faiss/build/release/repository/v1.0.0/linux_amd64/faiss.duckdb_extension'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME='my-nq-64-shot-example'\n",
    "# READER_MODEL = 'google/t5-base-lm-adapt '\n",
    "READER_MODEL = 't5-small'\n",
    "port=15000\n",
    "\n",
    "args = f'''--name 'my-nq-64-shot-example-evaluation' \n",
    "--generation_max_length 16 \n",
    "--gold_score_mode \"pdist\" \n",
    "--precision fp32 \n",
    "--reader_model_type {READER_MODEL} \n",
    "--text_maxlength 512 \n",
    "--model_path {MODEL_PATH} \n",
    "--eval_data {EVAL_PATH} \n",
    "--per_gpu_batch_size 1 \n",
    "--n_context 40 --retriever_n_context 40 \n",
    "--checkpoint_dir {MODEL_PATH} \n",
    "--main_port {port} \n",
    "--index_mode \"flat\" \n",
    "--task \"qa\" \n",
    "--load_index_path {FAISS_INDEX_PATH} \n",
    "--write_results'''\n",
    "\n",
    "args = ['/home/tkolb/RAG-Demo/atlas/evaluate.py'] + args.replace('\\'', '').replace('\\\"', '').replace('\\n', '').split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.18k/1.18k [00:00<00:00, 4.67MB/s]\n",
      "Downloading: 100%|██████████| 231M/231M [00:04<00:00, 58.3MB/s] \n",
      "Downloading: 100%|██████████| 2.27k/2.27k [00:00<00:00, 10.1MB/s]\n",
      "Downloading: 100%|██████████| 773k/773k [00:00<00:00, 2.37MB/s]\n",
      "Downloading: 100%|██████████| 1.32M/1.32M [00:00<00:00, 5.43MB/s]\n",
      "Some weights of the model checkpoint at facebook/contriever were not used when initializing Contriever: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing Contriever from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Contriever from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Atlas:\n\tMissing key(s) in state_dict: \"reader.encoder.block.0.layer.1.DenseReluDense.wi.weight\", \"reader.encoder.block.1.layer.1.DenseReluDense.wi.weight\", \"reader.encoder.block.2.layer.1.DenseReluDense.wi.weight\", \"reader.encoder.block.3.layer.1.DenseReluDense.wi.weight\", \"reader.encoder.block.4.layer.1.DenseReluDense.wi.weight\", \"reader.encoder.block.5.layer.1.DenseReluDense.wi.weight\", \"reader.decoder.block.0.layer.2.DenseReluDense.wi.weight\", \"reader.decoder.block.1.layer.2.DenseReluDense.wi.weight\", \"reader.decoder.block.2.layer.2.DenseReluDense.wi.weight\", \"reader.decoder.block.3.layer.2.DenseReluDense.wi.weight\", \"reader.decoder.block.4.layer.2.DenseReluDense.wi.weight\", \"reader.decoder.block.5.layer.2.DenseReluDense.wi.weight\". \n\tUnexpected key(s) in state_dict: \"reader.encoder.block.6.layer.0.SelfAttention.q.weight\", \"reader.encoder.block.6.layer.0.SelfAttention.k.weight\", \"reader.encoder.block.6.layer.0.SelfAttention.v.weight\", \"reader.encoder.block.6.layer.0.SelfAttention.o.weight\", \"reader.encoder.block.6.layer.0.layer_norm.weight\", \"reader.encoder.block.6.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.6.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.6.layer.1.DenseReluDense.wo.weight\", \"reader.encoder.block.6.layer.1.layer_norm.weight\", \"reader.encoder.block.7.layer.0.SelfAttention.q.weight\", \"reader.encoder.block.7.layer.0.SelfAttention.k.weight\", \"reader.encoder.block.7.layer.0.SelfAttention.v.weight\", \"reader.encoder.block.7.layer.0.SelfAttention.o.weight\", \"reader.encoder.block.7.layer.0.layer_norm.weight\", \"reader.encoder.block.7.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.7.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.7.layer.1.DenseReluDense.wo.weight\", \"reader.encoder.block.7.layer.1.layer_norm.weight\", \"reader.encoder.block.8.layer.0.SelfAttention.q.weight\", \"reader.encoder.block.8.layer.0.SelfAttention.k.weight\", \"reader.encoder.block.8.layer.0.SelfAttention.v.weight\", \"reader.encoder.block.8.layer.0.SelfAttention.o.weight\", \"reader.encoder.block.8.layer.0.layer_norm.weight\", \"reader.encoder.block.8.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.8.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.8.layer.1.DenseReluDense.wo.weight\", \"reader.encoder.block.8.layer.1.layer_norm.weight\", \"reader.encoder.block.9.layer.0.SelfAttention.q.weight\", \"reader.encoder.block.9.layer.0.SelfAttention.k.weight\", \"reader.encoder.block.9.layer.0.SelfAttention.v.weight\", \"reader.encoder.block.9.layer.0.SelfAttention.o.weight\", \"reader.encoder.block.9.layer.0.layer_norm.weight\", \"reader.encoder.block.9.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.9.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.9.layer.1.DenseReluDense.wo.weight\", \"reader.encoder.block.9.layer.1.layer_norm.weight\", \"reader.encoder.block.10.layer.0.SelfAttention.q.weight\", \"reader.encoder.block.10.layer.0.SelfAttention.k.weight\", \"reader.encoder.block.10.layer.0.SelfAttention.v.weight\", \"reader.encoder.block.10.layer.0.SelfAttention.o.weight\", \"reader.encoder.block.10.layer.0.layer_norm.weight\", \"reader.encoder.block.10.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.10.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.10.layer.1.DenseReluDense.wo.weight\", \"reader.encoder.block.10.layer.1.layer_norm.weight\", \"reader.encoder.block.11.layer.0.SelfAttention.q.weight\", \"reader.encoder.block.11.layer.0.SelfAttention.k.weight\", \"reader.encoder.block.11.layer.0.SelfAttention.v.weight\", \"reader.encoder.block.11.layer.0.SelfAttention.o.weight\", \"reader.encoder.block.11.layer.0.layer_norm.weight\", \"reader.encoder.block.11.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.11.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.11.layer.1.DenseReluDense.wo.weight\", \"reader.encoder.block.11.layer.1.layer_norm.weight\", \"reader.encoder.block.0.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.0.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.1.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.1.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.2.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.2.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.3.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.3.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.4.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.4.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.5.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.5.layer.1.DenseReluDense.wi_1.weight\", \"reader.decoder.block.6.layer.0.SelfAttention.q.weight\", \"reader.decoder.block.6.layer.0.SelfAttention.k.weight\", \"reader.decoder.block.6.layer.0.SelfAttention.v.weight\", \"reader.decoder.block.6.layer.0.SelfAttention.o.weight\", \"reader.decoder.block.6.layer.0.layer_norm.weight\", \"reader.decoder.block.6.layer.1.EncDecAttention.q.weight\", \"reader.decoder.block.6.layer.1.EncDecAttention.k.weight\", \"reader.decoder.block.6.layer.1.EncDecAttention.v.weight\", \"reader.decoder.block.6.layer.1.EncDecAttention.o.weight\", \"reader.decoder.block.6.layer.1.layer_norm.weight\", \"reader.decoder.block.6.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.6.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.6.layer.2.DenseReluDense.wo.weight\", \"reader.decoder.block.6.layer.2.layer_norm.weight\", \"reader.decoder.block.7.layer.0.SelfAttention.q.weight\", \"reader.decoder.block.7.layer.0.SelfAttention.k.weight\", \"reader.decoder.block.7.layer.0.SelfAttention.v.weight\", \"reader.decoder.block.7.layer.0.SelfAttention.o.weight\", \"reader.decoder.block.7.layer.0.layer_norm.weight\", \"reader.decoder.block.7.layer.1.EncDecAttention.q.weight\", \"reader.decoder.block.7.layer.1.EncDecAttention.k.weight\", \"reader.decoder.block.7.layer.1.EncDecAttention.v.weight\", \"reader.decoder.block.7.layer.1.EncDecAttention.o.weight\", \"reader.decoder.block.7.layer.1.layer_norm.weight\", \"reader.decoder.block.7.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.7.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.7.layer.2.DenseReluDense.wo.weight\", \"reader.decoder.block.7.layer.2.layer_norm.weight\", \"reader.decoder.block.8.layer.0.SelfAttention.q.weight\", \"reader.decoder.block.8.layer.0.SelfAttention.k.weight\", \"reader.decoder.block.8.layer.0.SelfAttention.v.weight\", \"reader.decoder.block.8.layer.0.SelfAttention.o.weight\", \"reader.decoder.block.8.layer.0.layer_norm.weight\", \"reader.decoder.block.8.layer.1.EncDecAttention.q.weight\", \"reader.decoder.block.8.layer.1.EncDecAttention.k.weight\", \"reader.decoder.block.8.layer.1.EncDecAttention.v.weight\", \"reader.decoder.block.8.layer.1.EncDecAttention.o.weight\", \"reader.decoder.block.8.layer.1.layer_norm.weight\", \"reader.decoder.block.8.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.8.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.8.layer.2.DenseReluDense.wo.weight\", \"reader.decoder.block.8.layer.2.layer_norm.weight\", \"reader.decoder.block.9.layer.0.SelfAttention.q.weight\", \"reader.decoder.block.9.layer.0.SelfAttention.k.weight\", \"reader.decoder.block.9.layer.0.SelfAttention.v.weight\", \"reader.decoder.block.9.layer.0.SelfAttention.o.weight\", \"reader.decoder.block.9.layer.0.layer_norm.weight\", \"reader.decoder.block.9.layer.1.EncDecAttention.q.weight\", \"reader.decoder.block.9.layer.1.EncDecAttention.k.weight\", \"reader.decoder.block.9.layer.1.EncDecAttention.v.weight\", \"reader.decoder.block.9.layer.1.EncDecAttention.o.weight\", \"reader.decoder.block.9.layer.1.layer_norm.weight\", \"reader.decoder.block.9.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.9.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.9.layer.2.DenseReluDense.wo.weight\", \"reader.decoder.block.9.layer.2.layer_norm.weight\", \"reader.decoder.block.10.layer.0.SelfAttention.q.weight\", \"reader.decoder.block.10.layer.0.SelfAttention.k.weight\", \"reader.decoder.block.10.layer.0.SelfAttention.v.weight\", \"reader.decoder.block.10.layer.0.SelfAttention.o.weight\", \"reader.decoder.block.10.layer.0.layer_norm.weight\", \"reader.decoder.block.10.layer.1.EncDecAttention.q.weight\", \"reader.decoder.block.10.layer.1.EncDecAttention.k.weight\", \"reader.decoder.block.10.layer.1.EncDecAttention.v.weight\", \"reader.decoder.block.10.layer.1.EncDecAttention.o.weight\", \"reader.decoder.block.10.layer.1.layer_norm.weight\", \"reader.decoder.block.10.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.10.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.10.layer.2.DenseReluDense.wo.weight\", \"reader.decoder.block.10.layer.2.layer_norm.weight\", \"reader.decoder.block.11.layer.0.SelfAttention.q.weight\", \"reader.decoder.block.11.layer.0.SelfAttention.k.weight\", \"reader.decoder.block.11.layer.0.SelfAttention.v.weight\", \"reader.decoder.block.11.layer.0.SelfAttention.o.weight\", \"reader.decoder.block.11.layer.0.layer_norm.weight\", \"reader.decoder.block.11.layer.1.EncDecAttention.q.weight\", \"reader.decoder.block.11.layer.1.EncDecAttention.k.weight\", \"reader.decoder.block.11.layer.1.EncDecAttention.v.weight\", \"reader.decoder.block.11.layer.1.EncDecAttention.o.weight\", \"reader.decoder.block.11.layer.1.layer_norm.weight\", \"reader.decoder.block.11.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.11.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.11.layer.2.DenseReluDense.wo.weight\", \"reader.decoder.block.11.layer.2.layer_norm.weight\", \"reader.decoder.block.0.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.0.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.1.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.1.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.2.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.2.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.3.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.3.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.4.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.4.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.5.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.5.layer.2.DenseReluDense.wi_1.weight\". \n\tsize mismatch for reader.shared.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([32128, 512]).\n\tsize mismatch for reader.encoder.embed_tokens.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([32128, 512]).\n\tsize mismatch for reader.encoder.block.0.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.0.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.0.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.0.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: copying a param with shape torch.Size([32, 12]) from checkpoint, the shape in current model is torch.Size([32, 8]).\n\tsize mismatch for reader.encoder.block.0.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.0.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.encoder.block.0.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.1.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.1.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.1.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.1.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.1.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.1.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.encoder.block.1.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.2.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.2.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.2.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.2.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.2.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.2.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.encoder.block.2.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.3.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.3.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.3.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.3.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.3.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.3.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.encoder.block.3.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.4.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.4.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.4.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.4.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.4.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.4.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.encoder.block.4.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.5.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.5.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.5.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.5.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.5.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.5.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.encoder.block.5.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.final_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.embed_tokens.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([32128, 512]).\n\tsize mismatch for reader.decoder.block.0.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.0.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.0.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.0.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: copying a param with shape torch.Size([32, 12]) from checkpoint, the shape in current model is torch.Size([32, 8]).\n\tsize mismatch for reader.decoder.block.0.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.0.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.0.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.0.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.0.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.0.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.0.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.decoder.block.0.layer.2.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.1.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.1.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.1.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.1.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.1.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.1.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.1.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.1.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.1.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.1.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.1.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.decoder.block.1.layer.2.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.2.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.2.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.2.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.2.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.2.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.2.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.2.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.2.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.2.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.2.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.2.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.decoder.block.2.layer.2.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.3.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.3.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.3.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.3.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.3.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.3.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.3.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.3.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.3.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.3.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.3.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.decoder.block.3.layer.2.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.4.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.4.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.4.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.4.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.4.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.4.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.4.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.4.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.4.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.4.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.4.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.decoder.block.4.layer.2.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.5.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.5.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.5.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.5.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.5.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.5.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.5.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.5.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.5.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.5.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.5.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.decoder.block.5.layer.2.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.final_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.lm_head.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([32128, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m slurm\u001b[38;5;241m.\u001b[39minit_signal_handler()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load ATLAS\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model, _, _, _, _, opt, step \u001b[38;5;241m=\u001b[39m \u001b[43mload_or_initialize_atlas_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     12\u001b[0m unwrapped_model \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mget_unwrapped_model_if_wrapped(model)\n",
      "File \u001b[0;32m~/RAG-Demo/atlas/src/model_io.py:191\u001b[0m, in \u001b[0;36mload_or_initialize_atlas_model\u001b[0;34m(opt, eval_only)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# fresh finetune run, initialized from old model\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     load_path, reset_params \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mmodel_path, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m model, optimizer, scheduler, retr_optimizer, retr_scheduler, opt_checkpoint, loaded_step \u001b[38;5;241m=\u001b[39m \u001b[43mload_atlas_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_only\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mload_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    195\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opt\u001b[38;5;241m.\u001b[39mmodel_path \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m loaded_step\n",
      "File \u001b[0;32m~/RAG-Demo/atlas/src/model_io.py:141\u001b[0m, in \u001b[0;36mload_atlas_model\u001b[0;34m(dir_path, opt, reset_params, eval_only)\u001b[0m\n\u001b[1;32m    138\u001b[0m retriever, retriever_tokenizer \u001b[38;5;241m=\u001b[39m load_retriever(opt, opt_checkpoint)\n\u001b[1;32m    140\u001b[0m model \u001b[38;5;241m=\u001b[39m Atlas(opt, reader, retriever, reader_tokenizer, retriever_tokenizer)\n\u001b[0;32m--> 141\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_atlas_model_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_only:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, opt_checkpoint, step\n",
      "File \u001b[0;32m~/RAG-Demo/atlas/src/model_io.py:122\u001b[0m, in \u001b[0;36m_load_atlas_model_state\u001b[0;34m(opt, opt_checkpoint, model, model_dict)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt\u001b[38;5;241m.\u001b[39muse_file_passages:  \u001b[38;5;66;03m# dont load retriever if in use_file_passages mode\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     model_dict \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m model_dict\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretriever\u001b[39m\u001b[38;5;124m\"\u001b[39m)}\n\u001b[0;32m--> 122\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m model \u001b[38;5;241m=\u001b[39m _cast_and_set_attrs_and_send_to_device(model, opt)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/scratch/tkolb/data/envs/rag/lib/python3.10/site-packages/torch/nn/modules/module.py:1497\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1492\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1493\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1494\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1498\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Atlas:\n\tMissing key(s) in state_dict: \"reader.encoder.block.0.layer.1.DenseReluDense.wi.weight\", \"reader.encoder.block.1.layer.1.DenseReluDense.wi.weight\", \"reader.encoder.block.2.layer.1.DenseReluDense.wi.weight\", \"reader.encoder.block.3.layer.1.DenseReluDense.wi.weight\", \"reader.encoder.block.4.layer.1.DenseReluDense.wi.weight\", \"reader.encoder.block.5.layer.1.DenseReluDense.wi.weight\", \"reader.decoder.block.0.layer.2.DenseReluDense.wi.weight\", \"reader.decoder.block.1.layer.2.DenseReluDense.wi.weight\", \"reader.decoder.block.2.layer.2.DenseReluDense.wi.weight\", \"reader.decoder.block.3.layer.2.DenseReluDense.wi.weight\", \"reader.decoder.block.4.layer.2.DenseReluDense.wi.weight\", \"reader.decoder.block.5.layer.2.DenseReluDense.wi.weight\". \n\tUnexpected key(s) in state_dict: \"reader.encoder.block.6.layer.0.SelfAttention.q.weight\", \"reader.encoder.block.6.layer.0.SelfAttention.k.weight\", \"reader.encoder.block.6.layer.0.SelfAttention.v.weight\", \"reader.encoder.block.6.layer.0.SelfAttention.o.weight\", \"reader.encoder.block.6.layer.0.layer_norm.weight\", \"reader.encoder.block.6.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.6.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.6.layer.1.DenseReluDense.wo.weight\", \"reader.encoder.block.6.layer.1.layer_norm.weight\", \"reader.encoder.block.7.layer.0.SelfAttention.q.weight\", \"reader.encoder.block.7.layer.0.SelfAttention.k.weight\", \"reader.encoder.block.7.layer.0.SelfAttention.v.weight\", \"reader.encoder.block.7.layer.0.SelfAttention.o.weight\", \"reader.encoder.block.7.layer.0.layer_norm.weight\", \"reader.encoder.block.7.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.7.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.7.layer.1.DenseReluDense.wo.weight\", \"reader.encoder.block.7.layer.1.layer_norm.weight\", \"reader.encoder.block.8.layer.0.SelfAttention.q.weight\", \"reader.encoder.block.8.layer.0.SelfAttention.k.weight\", \"reader.encoder.block.8.layer.0.SelfAttention.v.weight\", \"reader.encoder.block.8.layer.0.SelfAttention.o.weight\", \"reader.encoder.block.8.layer.0.layer_norm.weight\", \"reader.encoder.block.8.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.8.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.8.layer.1.DenseReluDense.wo.weight\", \"reader.encoder.block.8.layer.1.layer_norm.weight\", \"reader.encoder.block.9.layer.0.SelfAttention.q.weight\", \"reader.encoder.block.9.layer.0.SelfAttention.k.weight\", \"reader.encoder.block.9.layer.0.SelfAttention.v.weight\", \"reader.encoder.block.9.layer.0.SelfAttention.o.weight\", \"reader.encoder.block.9.layer.0.layer_norm.weight\", \"reader.encoder.block.9.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.9.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.9.layer.1.DenseReluDense.wo.weight\", \"reader.encoder.block.9.layer.1.layer_norm.weight\", \"reader.encoder.block.10.layer.0.SelfAttention.q.weight\", \"reader.encoder.block.10.layer.0.SelfAttention.k.weight\", \"reader.encoder.block.10.layer.0.SelfAttention.v.weight\", \"reader.encoder.block.10.layer.0.SelfAttention.o.weight\", \"reader.encoder.block.10.layer.0.layer_norm.weight\", \"reader.encoder.block.10.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.10.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.10.layer.1.DenseReluDense.wo.weight\", \"reader.encoder.block.10.layer.1.layer_norm.weight\", \"reader.encoder.block.11.layer.0.SelfAttention.q.weight\", \"reader.encoder.block.11.layer.0.SelfAttention.k.weight\", \"reader.encoder.block.11.layer.0.SelfAttention.v.weight\", \"reader.encoder.block.11.layer.0.SelfAttention.o.weight\", \"reader.encoder.block.11.layer.0.layer_norm.weight\", \"reader.encoder.block.11.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.11.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.11.layer.1.DenseReluDense.wo.weight\", \"reader.encoder.block.11.layer.1.layer_norm.weight\", \"reader.encoder.block.0.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.0.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.1.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.1.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.2.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.2.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.3.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.3.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.4.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.4.layer.1.DenseReluDense.wi_1.weight\", \"reader.encoder.block.5.layer.1.DenseReluDense.wi_0.weight\", \"reader.encoder.block.5.layer.1.DenseReluDense.wi_1.weight\", \"reader.decoder.block.6.layer.0.SelfAttention.q.weight\", \"reader.decoder.block.6.layer.0.SelfAttention.k.weight\", \"reader.decoder.block.6.layer.0.SelfAttention.v.weight\", \"reader.decoder.block.6.layer.0.SelfAttention.o.weight\", \"reader.decoder.block.6.layer.0.layer_norm.weight\", \"reader.decoder.block.6.layer.1.EncDecAttention.q.weight\", \"reader.decoder.block.6.layer.1.EncDecAttention.k.weight\", \"reader.decoder.block.6.layer.1.EncDecAttention.v.weight\", \"reader.decoder.block.6.layer.1.EncDecAttention.o.weight\", \"reader.decoder.block.6.layer.1.layer_norm.weight\", \"reader.decoder.block.6.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.6.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.6.layer.2.DenseReluDense.wo.weight\", \"reader.decoder.block.6.layer.2.layer_norm.weight\", \"reader.decoder.block.7.layer.0.SelfAttention.q.weight\", \"reader.decoder.block.7.layer.0.SelfAttention.k.weight\", \"reader.decoder.block.7.layer.0.SelfAttention.v.weight\", \"reader.decoder.block.7.layer.0.SelfAttention.o.weight\", \"reader.decoder.block.7.layer.0.layer_norm.weight\", \"reader.decoder.block.7.layer.1.EncDecAttention.q.weight\", \"reader.decoder.block.7.layer.1.EncDecAttention.k.weight\", \"reader.decoder.block.7.layer.1.EncDecAttention.v.weight\", \"reader.decoder.block.7.layer.1.EncDecAttention.o.weight\", \"reader.decoder.block.7.layer.1.layer_norm.weight\", \"reader.decoder.block.7.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.7.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.7.layer.2.DenseReluDense.wo.weight\", \"reader.decoder.block.7.layer.2.layer_norm.weight\", \"reader.decoder.block.8.layer.0.SelfAttention.q.weight\", \"reader.decoder.block.8.layer.0.SelfAttention.k.weight\", \"reader.decoder.block.8.layer.0.SelfAttention.v.weight\", \"reader.decoder.block.8.layer.0.SelfAttention.o.weight\", \"reader.decoder.block.8.layer.0.layer_norm.weight\", \"reader.decoder.block.8.layer.1.EncDecAttention.q.weight\", \"reader.decoder.block.8.layer.1.EncDecAttention.k.weight\", \"reader.decoder.block.8.layer.1.EncDecAttention.v.weight\", \"reader.decoder.block.8.layer.1.EncDecAttention.o.weight\", \"reader.decoder.block.8.layer.1.layer_norm.weight\", \"reader.decoder.block.8.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.8.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.8.layer.2.DenseReluDense.wo.weight\", \"reader.decoder.block.8.layer.2.layer_norm.weight\", \"reader.decoder.block.9.layer.0.SelfAttention.q.weight\", \"reader.decoder.block.9.layer.0.SelfAttention.k.weight\", \"reader.decoder.block.9.layer.0.SelfAttention.v.weight\", \"reader.decoder.block.9.layer.0.SelfAttention.o.weight\", \"reader.decoder.block.9.layer.0.layer_norm.weight\", \"reader.decoder.block.9.layer.1.EncDecAttention.q.weight\", \"reader.decoder.block.9.layer.1.EncDecAttention.k.weight\", \"reader.decoder.block.9.layer.1.EncDecAttention.v.weight\", \"reader.decoder.block.9.layer.1.EncDecAttention.o.weight\", \"reader.decoder.block.9.layer.1.layer_norm.weight\", \"reader.decoder.block.9.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.9.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.9.layer.2.DenseReluDense.wo.weight\", \"reader.decoder.block.9.layer.2.layer_norm.weight\", \"reader.decoder.block.10.layer.0.SelfAttention.q.weight\", \"reader.decoder.block.10.layer.0.SelfAttention.k.weight\", \"reader.decoder.block.10.layer.0.SelfAttention.v.weight\", \"reader.decoder.block.10.layer.0.SelfAttention.o.weight\", \"reader.decoder.block.10.layer.0.layer_norm.weight\", \"reader.decoder.block.10.layer.1.EncDecAttention.q.weight\", \"reader.decoder.block.10.layer.1.EncDecAttention.k.weight\", \"reader.decoder.block.10.layer.1.EncDecAttention.v.weight\", \"reader.decoder.block.10.layer.1.EncDecAttention.o.weight\", \"reader.decoder.block.10.layer.1.layer_norm.weight\", \"reader.decoder.block.10.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.10.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.10.layer.2.DenseReluDense.wo.weight\", \"reader.decoder.block.10.layer.2.layer_norm.weight\", \"reader.decoder.block.11.layer.0.SelfAttention.q.weight\", \"reader.decoder.block.11.layer.0.SelfAttention.k.weight\", \"reader.decoder.block.11.layer.0.SelfAttention.v.weight\", \"reader.decoder.block.11.layer.0.SelfAttention.o.weight\", \"reader.decoder.block.11.layer.0.layer_norm.weight\", \"reader.decoder.block.11.layer.1.EncDecAttention.q.weight\", \"reader.decoder.block.11.layer.1.EncDecAttention.k.weight\", \"reader.decoder.block.11.layer.1.EncDecAttention.v.weight\", \"reader.decoder.block.11.layer.1.EncDecAttention.o.weight\", \"reader.decoder.block.11.layer.1.layer_norm.weight\", \"reader.decoder.block.11.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.11.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.11.layer.2.DenseReluDense.wo.weight\", \"reader.decoder.block.11.layer.2.layer_norm.weight\", \"reader.decoder.block.0.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.0.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.1.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.1.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.2.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.2.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.3.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.3.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.4.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.4.layer.2.DenseReluDense.wi_1.weight\", \"reader.decoder.block.5.layer.2.DenseReluDense.wi_0.weight\", \"reader.decoder.block.5.layer.2.DenseReluDense.wi_1.weight\". \n\tsize mismatch for reader.shared.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([32128, 512]).\n\tsize mismatch for reader.encoder.embed_tokens.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([32128, 512]).\n\tsize mismatch for reader.encoder.block.0.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.0.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.0.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.0.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: copying a param with shape torch.Size([32, 12]) from checkpoint, the shape in current model is torch.Size([32, 8]).\n\tsize mismatch for reader.encoder.block.0.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.0.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.encoder.block.0.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.1.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.1.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.1.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.1.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.1.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.1.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.encoder.block.1.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.2.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.2.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.2.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.2.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.2.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.2.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.encoder.block.2.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.3.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.3.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.3.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.3.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.3.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.3.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.encoder.block.3.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.4.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.4.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.4.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.4.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.4.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.4.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.encoder.block.4.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.5.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.5.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.5.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.5.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.encoder.block.5.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.block.5.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.encoder.block.5.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.encoder.final_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.embed_tokens.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([32128, 512]).\n\tsize mismatch for reader.decoder.block.0.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.0.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.0.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.0.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: copying a param with shape torch.Size([32, 12]) from checkpoint, the shape in current model is torch.Size([32, 8]).\n\tsize mismatch for reader.decoder.block.0.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.0.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.0.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.0.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.0.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.0.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.0.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.decoder.block.0.layer.2.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.1.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.1.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.1.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.1.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.1.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.1.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.1.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.1.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.1.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.1.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.1.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.decoder.block.1.layer.2.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.2.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.2.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.2.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.2.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.2.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.2.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.2.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.2.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.2.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.2.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.2.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.decoder.block.2.layer.2.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.3.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.3.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.3.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.3.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.3.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.3.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.3.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.3.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.3.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.3.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.3.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.decoder.block.3.layer.2.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.4.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.4.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.4.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.4.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.4.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.4.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.4.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.4.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.4.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.4.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.4.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.decoder.block.4.layer.2.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.5.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.5.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.5.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.5.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.5.layer.0.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.5.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.5.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.5.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.5.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for reader.decoder.block.5.layer.1.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.block.5.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([768, 2048]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n\tsize mismatch for reader.decoder.block.5.layer.2.layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.decoder.final_layer_norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for reader.lm_head.weight: copying a param with shape torch.Size([32128, 768]) from checkpoint, the shape in current model is torch.Size([32128, 512])."
     ]
    }
   ],
   "source": [
    "# Load options\n",
    "sys.argv = args\n",
    "options = get_options()\n",
    "opt = options.parse()\n",
    "torch.manual_seed(opt.seed)\n",
    "slurm.init_distributed_mode(opt)\n",
    "slurm.init_signal_handler()\n",
    "\n",
    "# Load ATLAS\n",
    "model, _, _, _, _, opt, step = load_or_initialize_atlas_model(opt, eval_only=True)\n",
    "model.eval()\n",
    "unwrapped_model = util.get_unwrapped_model_if_wrapped(model)\n",
    "\n",
    "# Create data iterator from QA jsonl file\n",
    "reader_tokenizer = unwrapped_model.reader_tokenizer\n",
    "task = get_task(opt, reader_tokenizer)\n",
    "data_iterator = _get_eval_data_iterator(opt, opt.eval_data[0], task)\n",
    "\n",
    "# Get query encoding\n",
    "# TODO make it possible to do it for one query\n",
    "batch = data_iterator[0]\n",
    "query = batch.get(\"query\", [\"\"])\n",
    "answers = batch.get(\"target\", [\"\"])\n",
    "batch_metadata = batch.get(\"metadata\")\n",
    "query_enc, labels, decoder_input_ids = unwrapped_model.tokenize(query, answers, None)\n",
    "query_ids_retriever = query_enc[\"input_ids\"].cuda()\n",
    "query_mask_retriever = query_enc[\"attention_mask\"].cuda()\n",
    "\n",
    "# Get query embedding from model\n",
    "unwrapped_model.retriever.eval()\n",
    "query_emb = unwrapped_model.retriever(query_ids_retriever, query_mask_retriever, is_passages=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['question: who got the first nobel prize in physics answer: <extra_id_0>'],\n",
       " ['<extra_id_0> Wilhelm Conrad Röntgen'],\n",
       " torch.Size([1, 512]),\n",
       " torch.Size([1, 512]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query, answers, labels.shape, decoder_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────┐\n",
       "│ Success │\n",
       "│ boolean │\n",
       "├─────────┤\n",
       "│ 0 rows  │\n",
       "└─────────┘"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DuckDB session\n",
    "con = duckdb.connect(config = {'allow_unsigned_extensions': 'true'})\n",
    "\n",
    "# Load FAISS extension\n",
    "con.sql(f\"LOAD '{DUCKDB_FAISS_EXT}'\")\n",
    "\n",
    "# Load Wikipedia index (21GB)\n",
    "con.sql(f\"CALL faiss_load('index', '{FAISS_INDEX_PATH}');\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PASSAGES_PATH, 'rb') as f:\n",
    "    passages = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
       "│                                                        query                                                         │\n",
       "│                                                       double[]                                                       │\n",
       "├──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ [-0.14450009167194366, -0.019435692578554153, 0.041694700717926025, -0.08630520105361938, -0.03823167458176613, -0…  │\n",
       "└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_df = pd.DataFrame({\"query\": query_emb.tolist()})\n",
    "con.sql(\"SELECT * FROM query_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1321419/2867049774.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  top_k = [row[0] for _, row in topk_df.iterrows()]\n"
     ]
    }
   ],
   "source": [
    "topk_df = con.sql(f\"SELECT UNNEST(faiss_search('index', 40, query)) FROM query_df\").to_df()\n",
    "top_k = [row[0] for _, row in topk_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '27997088',\n",
       "  'title': '2011–12 Coppa Italia: =First round',\n",
       "  'section': '=First round',\n",
       "  'text': ' ='},\n",
       " {'id': '27997092',\n",
       "  'title': '2011–12 Coppa Italia: =First round',\n",
       "  'section': '=First round',\n",
       "  'text': ' ='},\n",
       " {'id': '27997089',\n",
       "  'title': '2011–12 Coppa Italia: =Second round',\n",
       "  'section': '=Second round',\n",
       "  'text': ' ='},\n",
       " {'id': '27997093',\n",
       "  'title': '2011–12 Coppa Italia: =Second round',\n",
       "  'section': '=Second round',\n",
       "  'text': ' ='},\n",
       " {'id': '20207299',\n",
       "  'title': '2010–11 Coppa Italia: =First round',\n",
       "  'section': '=First round',\n",
       "  'text': ' ='},\n",
       " {'id': '8972033',\n",
       "  'title': '2009–10 Coppa Italia: =First round',\n",
       "  'section': '=First round',\n",
       "  'text': ' ='},\n",
       " {'id': '8972037',\n",
       "  'title': '2009–10 Coppa Italia: =First round',\n",
       "  'section': '=First round',\n",
       "  'text': ' ='},\n",
       " {'id': '1612355',\n",
       "  'title': 'List of Swedish scientists',\n",
       "  'section': '',\n",
       "  'text': ' This is a list of Swedish scientists.'},\n",
       " {'id': '22764196',\n",
       "  'title': '1994 MTV Video Music Awards: Lifetime Achievement Award',\n",
       "  'section': 'Lifetime Achievement Award',\n",
       "  'text': ' The Rolling Stones'},\n",
       " {'id': '20550273',\n",
       "  'title': 'Henry Draper Medal: List of recipients',\n",
       "  'section': 'List of recipients',\n",
       "  'text': ' Source: National Academy of Sciences'},\n",
       " {'id': '6512355',\n",
       "  'title': '2008–09 Coppa Italia: =First Round',\n",
       "  'section': '=First Round',\n",
       "  'text': ' ='},\n",
       " {'id': '20207328',\n",
       "  'title': '2010–11 Coppa Italia: =Second round',\n",
       "  'section': '=Second round',\n",
       "  'text': ' ='},\n",
       " {'id': '20207300',\n",
       "  'title': '2010–11 Coppa Italia: =Second round',\n",
       "  'section': '=Second round',\n",
       "  'text': ' ='},\n",
       " {'id': '1932578',\n",
       "  'title': 'Odd Hassel',\n",
       "  'section': '',\n",
       "  'text': ' Odd Hassel (17 May 1897 &ndash; 11 May 1981) was a Norwegian physical chemist and Nobel Laureate.'},\n",
       " {'id': '16404804',\n",
       "  'title': 'Lake Gőtés',\n",
       "  'section': '',\n",
       "  'text': ' Lake Gőtés is a lake of Hungary.'},\n",
       " {'id': '8972034',\n",
       "  'title': '2009–10 Coppa Italia: =Second round',\n",
       "  'section': '=Second round',\n",
       "  'text': ' ='},\n",
       " {'id': '8972038',\n",
       "  'title': '2009–10 Coppa Italia: =Second round',\n",
       "  'section': '=Second round',\n",
       "  'text': ' ='},\n",
       " {'id': '509184',\n",
       "  'title': 'List of Kyoto Prize winners: Advanced technology',\n",
       "  'section': 'Advanced technology',\n",
       "  'text': ' Source: Kyoto Prize'},\n",
       " {'id': '16042278',\n",
       "  'title': 'Einstein tensor: Definition',\n",
       "  'section': 'Definition',\n",
       "  'text': ' The Einstein tensor'},\n",
       " {'id': 'infobox-3665125',\n",
       "  'title': 'Extravasation of urine',\n",
       "  'text': 'infobox name: Extravasation of urine'},\n",
       " {'id': '6512352',\n",
       "  'title': '2008–09 Coppa Italia: =Second Round',\n",
       "  'section': '=Second Round',\n",
       "  'text': ' ='},\n",
       " {'id': '6512356',\n",
       "  'title': '2008–09 Coppa Italia: =Second Round',\n",
       "  'section': '=Second Round',\n",
       "  'text': ' ='},\n",
       " {'id': '9405317',\n",
       "  'title': 'Prize of the Foundation for Polish Science: Recipients',\n",
       "  'section': 'Recipients',\n",
       "  'text': ' Source: Foundation for Polish Science'},\n",
       " {'id': 'infobox-755240',\n",
       "  'title': '2003 Nobel Peace Prize',\n",
       "  'text': 'infobox previous: 2002 ; main: Nobel Peace Prize ; next: 2004 ; date: 10 October 2003'},\n",
       " {'id': '4129729',\n",
       "  'title': \"2003 African Cup Winners' Cup: First round\",\n",
       "  'section': 'First round',\n",
       "  'text': '} '},\n",
       " {'id': '9633445',\n",
       "  'title': '1984–85 UEFA Cup: First round',\n",
       "  'section': 'First round',\n",
       "  'text': '} '},\n",
       " {'id': '8068643',\n",
       "  'title': \"1991 CONCACAF Champions' Cup: First Round\",\n",
       "  'section': 'First Round',\n",
       "  'text': '} '},\n",
       " {'id': '8765638',\n",
       "  'title': '1988–89 UEFA Cup: First round',\n",
       "  'section': 'First round',\n",
       "  'text': '} '},\n",
       " {'id': '10153634',\n",
       "  'title': '1977–78 UEFA Cup: First round',\n",
       "  'section': 'First round',\n",
       "  'text': '} '},\n",
       " {'id': '3375584',\n",
       "  'title': \"1992 African Cup Winners' Cup: First round\",\n",
       "  'section': 'First round',\n",
       "  'text': '} '},\n",
       " {'id': '27997090',\n",
       "  'title': '2011–12 Coppa Italia: =Third round',\n",
       "  'section': '=Third round',\n",
       "  'text': ' ='},\n",
       " {'id': '27997094',\n",
       "  'title': '2011–12 Coppa Italia: =Third round',\n",
       "  'section': '=Third round',\n",
       "  'text': ' ='},\n",
       " {'id': '9108931',\n",
       "  'title': \"1995–96 UEFA Cup Winners' Cup: First round\",\n",
       "  'section': 'First round',\n",
       "  'text': '} '},\n",
       " {'id': '16464003',\n",
       "  'title': \"1983 African Cup Winners' Cup: First round\",\n",
       "  'section': 'First round',\n",
       "  'text': '} '},\n",
       " {'id': '20016800',\n",
       "  'title': 'List of 1948 Summer Olympics medal winners: Swimming',\n",
       "  'section': 'Swimming',\n",
       "  'text': ' Men Women'},\n",
       " {'id': '2924133',\n",
       "  'title': \"1980–81 European Cup Winners' Cup: First round\",\n",
       "  'section': 'First round',\n",
       "  'text': '} '},\n",
       " {'id': '509185',\n",
       "  'title': 'List of Kyoto Prize winners: Arts and philosophy',\n",
       "  'section': 'Arts and philosophy',\n",
       "  'text': ' Source: Kyoto Prize'},\n",
       " {'id': '18693795',\n",
       "  'title': '2011–12 Bangladesh Federation Cup: Final',\n",
       "  'section': 'Final',\n",
       "  'text': '} '},\n",
       " {'id': '15839776',\n",
       "  'title': \"1978 African Cup Winners' Cup: First round\",\n",
       "  'section': 'First round',\n",
       "  'text': '} '},\n",
       " {'id': 'infobox-2293255',\n",
       "  'title': '1891 census of India',\n",
       "  'text': 'infobox name: 1891 Census of India ; country: British India'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_passages = []\n",
    "for dict_item in top_k:\n",
    "    id = int(dict_item['label'])\n",
    "    top_k_passages.append(passages[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search top k relevant documents for query embedding\n",
    "# Return top k document passages + scores\n",
    "def search_relevant_docs(query_emb, index_path, passages_path, k=5):\n",
    "    # DuckDB session\n",
    "    con = duckdb.connect(config = {'allow_unsigned_extensions': 'true'})\n",
    "\n",
    "    # Load FAISS extension\n",
    "    con.sql(f\"LOAD '{DUCKDB_FAISS_EXT}'\")\n",
    "\n",
    "    # Load Wikipedia index (21GB)\n",
    "    con.sql(f\"CALL faiss_load('index', '{FAISS_INDEX_PATH}');\")\n",
    "    \n",
    "    # Load Wikipedia passages\n",
    "    with open(passages_path, 'rb') as f:\n",
    "        passages = pickle.load(f)\n",
    "    \n",
    "    # Create query df and read into duckdb\n",
    "    query_df = pd.DataFrame({\"query\": query_emb.tolist()})\n",
    "    \n",
    "    # top k search\n",
    "    topk_df = con.sql(f\"SELECT UNNEST(faiss_search('index', {k}, query)) FROM query_df\").to_df()\n",
    "    top_k = [row[0] for _, row in topk_df.iterrows()]\n",
    "    \n",
    "    # Extend top k dictionaries with corresponding passages\n",
    "    top_k_passages = []\n",
    "    for dict_item in top_k:\n",
    "        id = int(dict_item['label'])\n",
    "        top_k_passages.append(passages[id])\n",
    "        \n",
    "    return top_k_passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1382415/804676433.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  top_k = [row[0] for _, row in topk_df.iterrows()]\n"
     ]
    }
   ],
   "source": [
    "top_k_passages = search_relevant_docs(query_emb, FAISS_INDEX_PATH, PASSAGES_PATH, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 60.00 MiB (GPU 1; 15.77 GiB total capacity; 14.35 GiB already allocated; 5.12 MiB free; 14.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m reader_mask_training \u001b[38;5;241m=\u001b[39m reader_mask_training\u001b[38;5;241m.\u001b[39mview(reader_mask\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Inference with reader ids\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m reader_output \u001b[38;5;241m=\u001b[39m \u001b[43munwrapped_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreader_ids_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreader_mask_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/tkolb/data/envs/rag/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/RAG-Demo/atlas/src/modeling_t5.py:1582\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;66;03m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1581\u001b[0m     \u001b[38;5;66;03m# Convert encoder inputs in embeddings if needed\u001b[39;00m\n\u001b[0;32m-> 1582\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[1;32m   1592\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1593\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1594\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1595\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1596\u001b[0m     )\n",
      "File \u001b[0;32m/scratch/tkolb/data/envs/rag/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/RAG-Demo/atlas/src/fid.py:51\u001b[0m, in \u001b[0;36mFiDStack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     48\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mview(input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_context, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mview(attention_mask\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_context, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/RAG-Demo/atlas/src/modeling_t5.py:1016\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m checkpoint(\n\u001b[1;32m   1004\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1005\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     )\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1016\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/tkolb/data/envs/rag/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/RAG-Demo/atlas/src/modeling_t5.py:701\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    698\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m attention_outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;66;03m# if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(hidden_states)\u001b[38;5;241m.\u001b[39many():\n",
      "File \u001b[0;32m/scratch/tkolb/data/envs/rag/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/RAG-Demo/atlas/src/modeling_t5.py:310\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    308\u001b[0m forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m    309\u001b[0m forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDenseReluDense(forwarded_states)\n\u001b[0;32m--> 310\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwarded_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 1; 15.77 GiB total capacity; 14.35 GiB already allocated; 5.12 MiB free; 14.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Move reader to cuda:1\n",
    "READER_DEVICE = 'cuda:1'\n",
    "unwrapped_model.reader.to(READER_DEVICE)\n",
    "\n",
    "# Create tokens for reader\n",
    "reader_tokens, retriever_tokens = unwrapped_model.tokenize_passages(query, [top_k_passages])\n",
    "reader_ids = reader_tokens[\"input_ids\"].to(READER_DEVICE)\n",
    "reader_mask = reader_tokens[\"attention_mask\"].bool().to(READER_DEVICE)\n",
    "n_context_training = min(unwrapped_model.opt.n_context, reader_ids.size(1))\n",
    "\n",
    "# Get reader config\n",
    "cfg = unwrapped_model.reader.encoder.config\n",
    "cfg.bsz = reader_ids.size(0)\n",
    "cfg.n_context = n_context_training\n",
    "\n",
    "# Reshape reader ids\n",
    "reader_ids_training = reader_ids[:, :n_context_training].contiguous()\n",
    "reader_mask_training = reader_mask[:, :n_context_training].contiguous()\n",
    "reader_ids_training = reader_ids_training.view(reader_ids.size(0), -1)\n",
    "reader_mask_training = reader_mask_training.view(reader_mask.size(0), -1)\n",
    "\n",
    "# Inference with reader ids\n",
    "reader_output = unwrapped_model.reader(\n",
    "    input_ids=reader_ids_training,\n",
    "    attention_mask=reader_mask_training,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    labels=labels,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
